import gym
import math
import random
import numpy as np
import matplotlib
import matplotlib.pyplot as plt
from collections import namedtuple
from itertools import count
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision.transforms as T
from collections import deque
import os
from snn import *
from functional import *
from utils import *
from torchvision import transforms
#from torchvision.transforms import ToPILImage
from PIL import Image

############ HYPERPARAMETERS ##############

BATCH_SIZE = 128
GAMMA = 0.999
EPS_START = 0.9
EPS_END = 0.05
EPS_DECAY = 1000
TARGET_UPDATE = 50
MEMORY_SIZE = 10000
END_SCORE = 200
TRAINING_STOP = 142
N_EPISODES = 500
LAST_EPISODES_NUM = 20
FRAMES = 2
RESIZE_PIXELS = 60

# ---- CONVOLUTIONAL NEURAL NETWORK ----
HIDDEN_LAYER_1 = 16
HIDDEN_LAYER_2 = 32
HIDDEN_LAYER_3 = 32
KERNEL_SIZE = 5
STRIDE = 2
# --------------------------------------

GRAYSCALE = True
LOAD_MODEL = False
USE_CUDA = False
############################################
if GRAYSCALE:
    nn_inputs = FRAMES  # should yield 2 (for grayscale)
else:
    nn_inputs = 3 * FRAMES 

kernels = [
    GaborKernel(window_size=3, orientation=45 + 22.5),
    GaborKernel(3, 90 + 22.5),
    GaborKernel(3, 135 + 22.5),
    GaborKernel(3, 180 + 22.5)
]
filt = Filter(kernels, use_abs=True)

def time_dim(input):
    return input.unsqueeze(0)  # Expand dimensions for Time, as needed

transform = transforms.Compose(
    [
        transforms.Grayscale(),  
        transforms.ToTensor(),  
        time_dim,  
        filt, 
        pointwise_inhibition,  # Apply inhibition
        Intensity2Latency(number_of_spike_bins=15, to_spike=True)  # Convert intensity to latency
    ]
)

graph_name = 'Cartpole_Vision_Stop-' + str(TRAINING_STOP) + '_LastEpNum-' + str(LAST_EPISODES_NUM)

'''if GRAYSCALE == 0:
    resize = T.Compose([T.ToPILImage(), 
                    T.Resize(RESIZE_PIXELS, interpolation=Image.BICUBIC),
                    T.ToTensor()])
    
    nn_inputs = 3*FRAMES  # number of channels for the nn
else:
    resize = T.Compose([T.ToPILImage(),
                    T.Resize(RESIZE_PIXELS, interpolation=Image.BICUBIC),
                    T.Grayscale(),
                    T.ToTensor()])
    nn_inputs =  FRAMES # number of channels for the nn'''

                    
stop_training = False 

env = gym.make('CartPole-v1', render_mode='rgb_array').unwrapped

# Set up matplotlib
is_ipython = 'inline' in matplotlib.get_backend()
if is_ipython:
    from IPython import display

plt.ion()

device = torch.device("cuda" if (torch.cuda.is_available() and USE_CUDA) else "cpu")

Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))

class ReplayMemory(object):
    def __init__(self, capacity):
        self.capacity = capacity
        self.memory = []
        self.position = 0

    def push(self, *args):
        """Saves a transition."""
        if len(self.memory) < self.capacity:
            self.memory.append(None)
        self.memory[self.position] = Transition(*args)  
        self.position = (self.position + 1) % self.capacity 

    def sample(self, batch_size):
        return random.sample(self.memory, batch_size) 

    def __len__(self): 
        return len(self.memory)

class SNN(nn.Module):
        def __init__(self, input_channels, features_per_class, number_of_classes):
            super(SNN, self).__init__()
            self.features_per_class = features_per_class
            self.number_of_classes = number_of_classes
            self.number_of_features = features_per_class * number_of_classes
            self.pool = Pooling(kernel_size=5, stride=1, padding=1)
            self.conv = Convolution(input_channels, self.number_of_features, 3, 0.8, 0.05)
            self.conv.reset_weight()
            self.stdp = STDP(conv_layer=self.conv, learning_rate=(0.05, -0.015))
            self.anti_stdp = STDP(conv_layer=self.conv, learning_rate=(-0.05, 0.0005))
            self.reduce_channels = nn.Conv2d(in_channels=8, out_channels=2, kernel_size=1)
        
        # internal state of the model
            self.ctx = {"input_spikes": None, "potentials": None, "output_spikes": None, "winners": None}
        
        # map each neuron to the class it represents
            self.decision_map = []
            for i in range(number_of_classes):
                self.decision_map.extend([i] * features_per_class)

    
        def forward(self, x):
            x = self.reduce_channels(x)
            x = self.pool(x)
            p = self.conv(x)
            spikes, potentials = fire(potentials=p, threshold=20, return_thresholded_potentials=True)
            winners = get_k_winners(potentials=p, kwta=1, inhibition_radius=0, spikes=spikes)
            self.ctx["input_spikes"] = x
            self.ctx["potentials"] = potentials
            self.ctx["output_spikes"] = spikes
            self.ctx["winners"] = winners

            output = -1
            if len(winners) != 0:
                output = self.decision_map[winners[0][0]]
                
            return output

        
        def reward(self, reward):
            if reward.item() > 3:
                self.stdp(self.ctx["input_spikes"], self.ctx["potentials"], self.ctx["output_spikes"], self.ctx["winners"])
            elif reward.item() < 5:
                self.anti_stdp(self.ctx["input_spikes"], self.ctx["potentials"], self.ctx["output_spikes"], self.ctx["winners"])

def get_cart_location(screen_width):
    world_width = env.x_threshold * 2
    scale = screen_width / world_width
    return int(env.state[0] * scale + screen_width / 2.0)


def get_screen():
    # Initialize a deque to hold transformed images
    transformed_screens = deque(maxlen=2)

    # Capture two screenshots and transform them
    for _ in range(2):
        # Get the current screen
        screen = env.render().transpose((2, 0, 1))  # Capture and transpose to (channels, height, width)
        
        _, screen_height, screen_width = screen.shape
        screen = screen[:, int(screen_height * 0.4):int(screen_height * 0.8)]  # Crop the screen

        # Define the view width as 60% of the screen width
        view_width = int(screen_width * 0.6)
        cart_location = get_cart_location(screen_width)
        
        # Determine the slice range based on the cart's location
        if cart_location < view_width // 2:
            slice_range = slice(view_width)
        elif cart_location > (screen_width - view_width // 2):
            slice_range = slice(-view_width, None)
        else:
            slice_range = slice(cart_location - view_width // 2, cart_location + view_width // 2)

        screen = screen[:, :, slice_range]  # Slice the screen accordingly
        screen = np.ascontiguousarray(screen.transpose(1, 2, 0), dtype=np.uint8)  # Rearrange to (height, width, channels)    
        image = Image.fromarray(screen[:, :, 0])  # Handle single channel images

        # Transform the image
        transformed_state = transform(image)
        print('transformed_state',transformed_state.shape)
        screens = deque([transformed_state] * FRAMES, FRAMES)
        state = torch.cat(list(screens), dim=1)
        print('state_size', state.shape)

    return state


env.reset()
'''plt.figure()
plt.imshow(get_screen().cpu().squeeze(0).permute(1, 2, 0).numpy(), cmap='gray')
plt.title('Example extracted screen')
plt.show()'''

eps_threshold = 0.9

init_screen = get_screen()
_, _, screen_height, screen_width = np.array(init_screen).shape
print("Screen height: ", screen_height, " | Width: ", screen_width)

# Get number of actions from gym action space
n_actions = env.action_space.n

# Initialize policy and target networks
model = SNN(nn_inputs, 20, 2)

memory = ReplayMemory(MEMORY_SIZE)

def select_action(state):
    global steps_done
    eps_threshold = EPS_END + (EPS_START - EPS_END) * \
        math.exp(-1. * steps_done / EPS_DECAY)
    steps_done += 1
    
    # Use the SNN to get action
    action = model.forward(state)
    
    # Decide based on the exploration strategy
    if random.random() > eps_threshold:
        return action  # Exploit the SNN output
    else:
        return random.randrange(n_actions)  # Explore with a random action


steps_done = 0

episode_durations = []

def plot_durations(score):
    plt.figure(2)
    plt.clf()
    durations_t = torch.tensor(episode_durations, dtype=torch.float)
    episode_number = len(durations_t) 
    plt.title('Training...')
    plt.xlabel('Episode')
    plt.ylabel('Duration')
    plt.plot(durations_t.numpy(), label='Score')
    matplotlib.pyplot.hlines(195, 0, episode_number, colors='red', linestyles=':', label='Win Threshold')
    if len(durations_t) >= 100:
        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)
        last100_mean = means[episode_number - 100].item()
        means = torch.cat((torch.zeros(99), means))
        plt.plot(means.numpy(), label='Last 100 mean')
        print('Episode: ', episode_number, ' | Score: ', score, '| Last 100 mean = ', last100_mean)
    plt.legend(loc='upper left')
    plt.pause(0.001)
    if is_ipython:
        display.clear_output(wait=True)
        display.display(plt.gcf())



mean_last = deque([0] * LAST_EPISODES_NUM, LAST_EPISODES_NUM)

# MAIN LOOP
screens = get_screen()

for i_episode in range(N_EPISODES):
    env.reset()
    #if len(screens) > 2:
     #   screens = list(screens)[-2:]  
        
    # Create the state tensor by concatenating frames
    state = screens
    #print('state', state)
    #print('state_size', state.shape)
    
    
    for t in count():
        action = select_action(state)  # Ensure `state` has correct dimensions
        state_variables, _, done, _, _ = env.step(action.item())  # Step in the environment
        next_state = get_screen()
        # Calculate reward
        position, velocity, angle, angular_velocity = state_variables
        r1 = (env.x_threshold - abs(position)) / env.x_threshold - 0.8
        r2 = (env.theta_threshold_radians - abs(angle)) / env.theta_threshold_radians - 0.5
        reward = r1 + r2
        reward = torch.tensor([reward], device=device)
        model.reward(reward)

        # Push the transition to memory
        memory.push(state, action, next_state, reward)  
        state = next_state  # Move to next state

        if done:
            episode_durations.append(t + 1)
            plot_durations(t + 1)
            break


print('Complete')
env.render()
env.close()
plt.ioff()
plt.show()